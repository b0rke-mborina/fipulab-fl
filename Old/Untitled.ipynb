{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88b1ad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81e2d1d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python run_ner.py --data_dir=data/ --bert_model=bert-base-cased --output_dir=model_sep20 --max_seq_length=128 --do_train --num_train_epochs 3 --do_eval --eval_on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1f917b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from bert import Ner\n",
    "from run_ner import NerProcessor\n",
    "import argparse\n",
    "from model import BertNer\n",
    "from fastprogress import master_bar, progress_bar\n",
    "from tokenization import FullTokenizer\n",
    "from run_ner import *\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66615f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python run_ner.py --data_dir=data/ --bert_model=bert-base-cased --output_dir=model_sep20 --max_seq_length=128 --num_train_epochs 3 --do_eval --eval_on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cd9ef288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_examples = NerProcessor.get_train_examples(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c2ae5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_label(filename):\n",
    "    f = open(filename)\n",
    "    split_labeled_text = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                split_labeled_text.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append([splits[0], splits[-1].rstrip(\"\\n\")])\n",
    "    if len(sentence) > 0:\n",
    "        split_labeled_text.append(sentence)\n",
    "        sentence = []\n",
    "    return split_labeled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc7bd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train = split_text_label(os.path.join('data', \"train.txt\"))\n",
    "split_valid = split_text_label(os.path.join('data', \"valid.txt\"))\n",
    "split_test = split_text_label(os.path.join('data', \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a29869",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "wordSet = set()\n",
    "# words and labels\n",
    "for data in [split_train, split_valid, split_test]:\n",
    "    for labeled_text in data:\n",
    "        for word, label in labeled_text:\n",
    "            labelSet.add(label)\n",
    "            wordSet.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e894f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the set to ensure '0' is assigned to 0\n",
    "sorted_labels = sorted(list(labelSet), key=len)\n",
    "# Create mapping for labels\n",
    "label2Idx = {}\n",
    "for label in sorted_labels:\n",
    "    label2Idx[label] = len(label2Idx)\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "# Create mapping for words\n",
    "word2Idx = {}\n",
    "if len(word2Idx) == 0:\n",
    "    word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "    word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "for word in wordSet:\n",
    "    word2Idx[word] = len(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8234a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(data, word2Idx, label2Idx):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for split_labeled_text in data:\n",
    "        wordIndices = []\n",
    "        labelIndices = []\n",
    "        for word, label in split_labeled_text:\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "            wordIndices.append(wordIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "        sentences.append(wordIndices)\n",
    "        labels.append(labelIndices)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "341e5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels = createMatrices(split_train, word2Idx, label2Idx)\n",
    "valid_sentences, valid_labels = createMatrices(split_valid, word2Idx, label2Idx)\n",
    "test_sentences, test_labels = createMatrices(split_test, word2Idx, label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ed1f00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentences, labels, max_len, padding='post'):\n",
    "    padded_sentences = pad_sequences(sentences, max_len, padding='post')\n",
    "    padded_labels = pad_sequences(labels, max_len, padding='post')\n",
    "    return padded_sentences, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d02ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len=16\n",
    "train_features, train_labels = padding(train_sentences, train_labels, max_seq_len, padding='post')\n",
    "valid_features, valid_labels = padding(valid_sentences, valid_labels, max_seq_len, padding='post')\n",
    "test_features, test_labels = padding(test_sentences, test_labels, max_seq_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f76a373f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1449700/3804703880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_input_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_segment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_valid_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_label_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1449700/3804703880.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_input_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_segment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_valid_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_label_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'input_ids'"
     ]
    }
   ],
   "source": [
    "# all_input_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.input_ids for f in train_features]))\n",
    "# all_input_mask = tf.data.Dataset.from_tensor_slices(np.asarray([f.input_mask for f in train_features]))\n",
    "# all_segment_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.segment_ids for f in train_features]))\n",
    "# all_valid_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.valid_ids for f in train_features]))\n",
    "# all_label_mask = tf.data.Dataset.from_tensor_slices(np.asarray([f.label_mask for f in train_features]))\n",
    "# all_label_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.label_id for f in train_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4beaef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load tokenizer\n",
    "# tokenizer = FullTokenizer(os.path.join(args.output_dir, \"vocab.txt\"), args.do_lower_case)\n",
    "# # model build hack : fix\n",
    "# config = json.load(open(os.path.join(args.output_dir, \"bert_config.json\")))\n",
    "# ner = BertNer(config, tf.float32, num_labels, \n",
    "# args.max_seq_length)\n",
    "# ids = tf.ones((1,128),dtype=tf.int64)\n",
    "# _ = ner(ids,ids,ids,ids, training=False)\n",
    "# ner.load_weights(os.path.join(args.output_dir,\"model.h5\"))\n",
    "# # load test or development set based on argsK\n",
    "# if args.eval_on == \"dev\":\n",
    "#     eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "# elif args.eval_on == \"test\":\n",
    "#     eval_examples = processor.get_test_examples(args.data_dir)\n",
    "#     eval_features = convert_examples_to_features(eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "#     all_input_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.input_ids for f in eval_features]))  \n",
    "#     all_input_mask = tf.data.Dataset.from_tensor_slices(np.asarray([f.input_mask for f in eval_features]))\n",
    "#     all_segment_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.segment_ids for f in eval_features]))\n",
    "#     all_valid_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.valid_ids for f in eval_features]))\n",
    "#     all_label_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.label_id for f in eval_features]))\n",
    "#     eval_data = tf.data.Dataset.zip((all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids))\n",
    "#     batched_eval_data = eval_data.batch(args.eval_batch_size)\n",
    "\n",
    "# loss_metric = tf.keras.metrics.Mean()\n",
    "# epoch_bar = master_bar(range(1))\n",
    "# pb_max_len = math.ceil(\n",
    "# float(len(eval_features))/float(args.eval_batch_size))\n",
    "# y_true = []\n",
    "# y_pred = []\n",
    "# label_map = {i : label for i, label in enumerate(label_list,1)}\n",
    "# for epoch in epoch_bar:\n",
    "#   for (input_ids, input_mask, segment_ids, valid_ids, label_ids) in progress_bar(batched_eval_data, total=pb_max_len, parent=epoch_bar):\n",
    "#     logits = ner(input_ids, input_mask, segment_ids, valid_ids, training=False)\n",
    "#     logits = tf.argmax(logits,axis=2)\n",
    "#     for i, label in enumerate(label_ids):\n",
    "#      temp_1 = []\n",
    "#      temp_2 = []\n",
    "#      for j,m in enumerate(label):\n",
    "#        if j == 0:\n",
    "#          continue\n",
    "#        elif label_ids[i][j].numpy() == len(label_map):     \n",
    "#          y_true.append(temp_1)\n",
    "#          y_pred.append(temp_2)\n",
    "#          break\n",
    "#        else:\n",
    "#          temp_1.append(label_map[label_ids[i][j].numpy()])\n",
    "#          temp_2.append(label_map[logits[i][j].numpy()])\n",
    "\n",
    "# report = classification_report(y_true, y_pred,digits=4)\n",
    "# output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "# with open(output_eval_file, \"w\") as writer:\n",
    "#   logger.info(\"***** Eval results *****\")\n",
    "#   logger.info(\"\\n%s\", report)\n",
    "# writer.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de7b5032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids,label_mask):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = ner(input_ids, input_mask,segment_ids, valid_ids, \n",
    "    training=True) #batchsize, max_seq_length, num_labels\n",
    "    label_ids_masked = tf.boolean_mask(label_ids,label_mask)        \n",
    "    logits_masked = tf.boolean_mask(logits,label_mask)\n",
    "    loss = loss_fct(label_ids_masked, logits_masked)\n",
    "  grads = tape.gradient(loss, ner.trainable_variables)\n",
    "  optimizer.apply_gradients(list(zip(grads,       \n",
    "  ner.trainable_variables)))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d7059a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in epoch_bar:\n",
    "#     for (input_ids, input_mask, segment_ids, valid_ids, \n",
    "#     label_ids,label_mask) in progress_bar(batched_train_data, \n",
    "#     total=pb_max_len, parent=epoch_bar):\n",
    "#       loss = train_step(input_ids, input_mask, segment_ids,\n",
    "#       valid_ids, label_ids,label_mask)\n",
    "#       loss_metric(loss)\n",
    "#       epoch_bar.child.comment = f'loss : {loss_metric.result()}'\n",
    "#     loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "302c1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validation Features (text)\n",
    "# X_train_val, X_test = pd.concat([split_train['text'], val_df['text']]).values, test_df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88fe0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and Validation Labels\n",
    "# y_train_val = label_encoder.fit_transform(pd.concat([train_df['label'], val_df['label']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3c62abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into new train and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.1, random_state=0, stratify=y_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004ce1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
