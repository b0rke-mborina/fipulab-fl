{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ea82a6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (1.19.5)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (12.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (0.23.1)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (1.34.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (0.13.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (3.17.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.32.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (0.36.2)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (3.7.4.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (1.32.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (56.0.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.5.30)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/fipulab/.pyenv/versions/3.8.10/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/fipulab/.pyenv/versions/3.8.10/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install pyspark\n",
    "# ! pip install --ignore-installed pyspark==2.4.4\n",
    "# Install Spark NLP\n",
    "# ! pip install --ignore-installed spark-nlp\n",
    "# Install tensorflow\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "423f39ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89b6303",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "Dataset preuzet s: https://deepai.org/dataset/conll-2003-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d10c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_label(filename):\n",
    "    f = open(filename)\n",
    "    split_labeled_text = []\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        if len(line) == 0 or line.startswith('-DOCSTART') or line[0] == \"\\n\":\n",
    "            if len(sentence) > 0:\n",
    "                split_labeled_text.append(sentence)\n",
    "                sentence = []\n",
    "            continue\n",
    "        splits = line.split(' ')\n",
    "        sentence.append([splits[0], splits[-1].rstrip(\"\\n\")])\n",
    "    if len(sentence) > 0:\n",
    "        split_labeled_text.append(sentence)\n",
    "        sentence = []\n",
    "    return split_labeled_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d8b68ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train = split_text_label(os.path.join('conll2003', \"train.txt\"))\n",
    "split_valid = split_text_label(os.path.join('conll2003', \"valid.txt\"))\n",
    "split_test = split_text_label(os.path.join('conll2003', \"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea583bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labelSet = set()\n",
    "wordSet = set()\n",
    "# words and labels\n",
    "for data in [split_train, split_valid, split_test]:\n",
    "    for labeled_text in data:\n",
    "        for word, label in labeled_text:\n",
    "            labelSet.add(label)\n",
    "            wordSet.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15c557ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the set to ensure '0' is assigned to 0\n",
    "sorted_labels = sorted(list(labelSet), key=len)\n",
    "# Create mapping for labels\n",
    "label2Idx = {}\n",
    "for label in sorted_labels:\n",
    "    label2Idx[label] = len(label2Idx)\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "# Create mapping for words\n",
    "word2Idx = {}\n",
    "if len(word2Idx) == 0:\n",
    "    word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "    word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "for word in wordSet:\n",
    "    word2Idx[word] = len(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35dda39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMatrices(data, word2Idx, label2Idx):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for split_labeled_text in data:\n",
    "        wordIndices = []\n",
    "        labelIndices = []\n",
    "        for word, label in split_labeled_text:\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            elif word.lower() in word2Idx:\n",
    "                wordIdx = word2Idx[word.lower()]\n",
    "            else:\n",
    "                wordIdx = word2Idx['UNKNOWN_TOKEN']\n",
    "            wordIndices.append(wordIdx)\n",
    "            labelIndices.append(label2Idx[label])\n",
    "        sentences.append(wordIndices)\n",
    "        labels.append(labelIndices)\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "141b52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels = createMatrices(split_train, word2Idx, label2Idx)\n",
    "valid_sentences, valid_labels = createMatrices(split_valid, word2Idx, label2Idx)\n",
    "test_sentences, test_labels = createMatrices(split_test, word2Idx, label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4901dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentences, labels, max_len, padding='post'):\n",
    "    padded_sentences = pad_sequences(sentences, max_len, padding='post')\n",
    "    padded_labels = pad_sequences(labels, max_len, padding='post')\n",
    "    return padded_sentences, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f66c43dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--max_seq_length\",\n",
    "#                     default=128,\n",
    "#                     type=int,\n",
    "#                     help=\"The maximum total input sequence length after WordPiece tokenization. \\n\"\n",
    "#                          \"Sequences longer than this will be truncated, and sequences shorter \\n\"\n",
    "#                          \"than this will be padded.\")\n",
    "# args = parser.parse_args()\n",
    "# processor = NerProcessor()\n",
    "# # Required parameters\n",
    "# parser.add_argument(\"--data_dir\",\n",
    "#                     default=None,\n",
    "#                     type=str,\n",
    "#                     required=True,\n",
    "#                     help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "# parser.add_argument(\"--bert_model\", default=None, type=str, required=True,\n",
    "#                     help=\"Bert pre-trained model selected in the list: bert-base-cased,bert-large-cased\")\n",
    "# parser.add_argument(\"--output_dir\",\n",
    "#                     default=None,\n",
    "#                     type=str,\n",
    "#                     required=True,\n",
    "#                     help=\"The output directory where the model predictions and checkpoints will be written.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ff1eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = padding(train_sentences, train_labels, max_seq_len, padding='post')\n",
    "valid_features, valid_labels = padding(valid_sentences, valid_labels, max_seq_len, padding='post')\n",
    "test_features, test_labels = padding(test_sentences, test_labels, max_seq_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbf77661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_examples = processor.get_train_examples(args.data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76e8e840",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1403453/3804703880.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_input_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_segment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_valid_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_label_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1403453/3804703880.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_input_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_input_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_segment_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msegment_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_valid_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_ids\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_label_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_mask\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'input_ids'"
     ]
    }
   ],
   "source": [
    "all_input_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.input_ids for f in train_features]))\n",
    "all_input_mask = tf.data.Dataset.from_tensor_slices(np.asarray([f.input_mask for f in train_features]))\n",
    "all_segment_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.segment_ids for f in train_features]))\n",
    "all_valid_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.valid_ids for f in train_features]))\n",
    "all_label_mask = tf.data.Dataset.from_tensor_slices(np.asarray([f.label_mask for f in train_features]))\n",
    "all_label_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.label_id for f in train_features]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "744ac8e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_input_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1403453/105229506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Dataset using tf.data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_input_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_segment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_valid_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_label_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_label_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mshuffled_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshuffle_each_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatched_train_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffled_train_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_input_ids' is not defined"
     ]
    }
   ],
   "source": [
    "# Dataset using tf.data\n",
    "train_data = tf.data.Dataset.zip((all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids,all_label_mask))\n",
    "shuffled_train_data = train_data.shuffle(buffer_size=int(len(train_features) * 0.1),seed = args.seed, reshuffle_each_iteration=True)\n",
    "batched_train_data = shuffled_train_data.batch(args.train_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55a2533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b316ccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertNer(tf.keras.Model):\n",
    "  def __init__(self, bert_model,float_type, num_labels,   \n",
    "  max_seq_length, final_layer_initializer=None):\n",
    "    super(BertNer, self).__init__()\n",
    "    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,),   \n",
    "    dtype=tf.int32, name='input_word_ids')\n",
    "    input_mask = tf.keras.layers.Input(shape=(max_seq_length,),     \n",
    "    dtype=tf.int32, name='input_mask')\n",
    "    input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), \n",
    "    dtype=tf.int32, name='input_type_ids')\n",
    "    \n",
    "    bert_config = BertConfig.from_json_file (os.path.join(bert_model, \"bert_config.json\"))\n",
    "    bert_layer = BertModel(config=bert_config,\n",
    "      float_type=float_type)\n",
    "    _, sequence_output = bert_layer(input_word_ids,  \n",
    "    input_mask,input_type_ids)\n",
    "    self.bert = tf.keras.Model(inputs=[input_word_ids, input_mask,  \n",
    "    input_type_ids],outputs=[sequence_output])\n",
    "    if type(bert_model) == str:\n",
    "      init_checkpoint = os.path.join(bert_model,\"bert_model.ckpt\")     \n",
    "      checkpoint = tf.train.Checkpoint(model=self.bert)\n",
    "      checkpoint.restore(init_checkpoint).assert_\n",
    "      existing_objects_matched()\n",
    "   \n",
    "    self.dropout = tf.keras.layers.Dropout(    \n",
    "    rate=bert_config.hidden_dropout_prob)\n",
    "    if final_layer_initializer is not None: \n",
    "      initializer = final_layer_initializer\n",
    "    else:\n",
    "      initializer = tf.keras.initializers.TruncatedNormal(   \n",
    "      stddev=bert_config.initializer_range)\n",
    "    self.classifier = tf.keras.layers.Dense(num_labels, \n",
    "    kernel_initializer=initializer, name='output', dtype=float_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "774e81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call(self, input_word_ids,input_mask=None,input_type_ids=None,\n",
    "valid_ids=None, **kwargs):\n",
    "    sequence_output = self.bert([input_word_ids, input_mask,  \n",
    "    input_type_ids],**kwargs)\n",
    "    valid_output = []\n",
    "    for i in range(sequence_output.shape[0]): \n",
    "      r = 0\n",
    "      temp = []\n",
    "      for j in range(sequence_output.shape[1]):\n",
    "        if valid_ids[i][j] == 1:\n",
    "           temp = temp + [sequence_output[i][j]]\n",
    "        else:\n",
    "           r += 1\n",
    "      temp = temp + r * [tf.zeros_like(sequence_output[i][j])]\n",
    "      valid_output = valid_output + temp\n",
    "    valid_output = tf.reshape(tf.stack(valid_output)\n",
    "    ,sequence_output.shape)\n",
    "    sequence_output = self.dropout(valid_output,  \n",
    "    training=kwargs.get('training', False)) \n",
    "    logits = self.classifier(sequence_output)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab04fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80576d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_ids, input_mask, segment_ids, valid_ids, label_ids,label_mask):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = ner(input_ids, input_mask,segment_ids, valid_ids, \n",
    "    training=True) #batchsize, max_seq_length, num_labels\n",
    "    label_ids_masked = tf.boolean_mask(label_ids,label_mask)        \n",
    "    logits_masked = tf.boolean_mask(logits,label_mask)\n",
    "    loss = loss_fct(label_ids_masked, logits_masked)\n",
    "  grads = tape.gradient(loss, ner.trainable_variables)\n",
    "  optimizer.apply_gradients(list(zip(grads,       \n",
    "  ner.trainable_variables)))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "519e2d28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FullTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1403453/3943919197.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# if args.do_eval:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# load tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFullTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"vocab.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# model build hack : fix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bert_config.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FullTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluation on valid dataset\n",
    "# if args.do_eval:\n",
    "# load tokenizer\n",
    "tokenizer = FullTokenizer(os.path.join(args.output_dir, \"vocab.txt\"), args.do_lower_case)\n",
    "# model build hack : fix\n",
    "config = json.load(open(os.path.join(args.output_dir, \"bert_config.json\")))\n",
    "ner = BertNer(config, tf.float32, num_labels, \n",
    "args.max_seq_length)\n",
    "ids = tf.ones((1,128),dtype=tf.int64)\n",
    "_ = ner(ids,ids,ids,ids, training=False)\n",
    "ner.load_weights(os.path.join(args.output_dir,\"model.h5\"))\n",
    "# load test or development set based on argsK\n",
    "if args.eval_on == \"dev\":\n",
    "    eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "elif args.eval_on == \"test\":\n",
    "    eval_examples = processor.get_test_examples(args.data_dir)\n",
    "    eval_features = convert_examples_to_features(eval_examples, label_list, args.max_seq_length, tokenizer)\n",
    "    all_input_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.input_ids for f in eval_features]))  \n",
    "    all_input_mask = tf.data.Dataset.from_tensor_slices(np.asarray([f.input_mask for f in eval_features]))\n",
    "    all_segment_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.segment_ids for f in eval_features]))\n",
    "    all_valid_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.valid_ids for f in eval_features]))\n",
    "    all_label_ids = tf.data.Dataset.from_tensor_slices(np.asarray([f.label_id for f in eval_features]))\n",
    "    eval_data = tf.data.Dataset.zip((all_input_ids, all_input_mask, all_segment_ids, all_valid_ids, all_label_ids))\n",
    "    batched_eval_data = eval_data.batch(args.eval_batch_size)\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "epoch_bar = master_bar(range(1))\n",
    "pb_max_len = math.ceil(\n",
    "float(len(eval_features))/float(args.eval_batch_size))\n",
    "y_true = []\n",
    "y_pred = []\n",
    "label_map = {i : label for i, label in enumerate(label_list,1)}\n",
    "for epoch in epoch_bar:\n",
    "  for (input_ids, input_mask, segment_ids, valid_ids, label_ids) in progress_bar(batched_eval_data, total=pb_max_len, parent=epoch_bar):\n",
    "    logits = ner(input_ids, input_mask, segment_ids, valid_ids, training=False)\n",
    "    logits = tf.argmax(logits,axis=2)\n",
    "    for i, label in enumerate(label_ids):\n",
    "     temp_1 = []\n",
    "     temp_2 = []\n",
    "     for j,m in enumerate(label):\n",
    "       if j == 0:\n",
    "         continue\n",
    "       elif label_ids[i][j].numpy() == len(label_map):     \n",
    "         y_true.append(temp_1)\n",
    "         y_pred.append(temp_2)\n",
    "         break\n",
    "       else:\n",
    "         temp_1.append(label_map[label_ids[i][j].numpy()])\n",
    "         temp_2.append(label_map[logits[i][j].numpy()])\n",
    "\n",
    "report = classification_report(y_true, y_pred,digits=4)\n",
    "output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "with open(output_eval_file, \"w\") as writer:\n",
    "  logger.info(\"***** Eval results *****\")\n",
    "  logger.info(\"\\n%s\", report)\n",
    "writer.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "abce8bd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epoch_bar' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1403453/992349684.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     for (input_ids, input_mask, segment_ids, valid_ids, \n\u001b[1;32m      3\u001b[0m     \u001b[0mlabel_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_train_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     total=pb_max_len, parent=epoch_bar):\n\u001b[1;32m      5\u001b[0m       loss = train_step(input_ids, input_mask, segment_ids,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_bar' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in epoch_bar:\n",
    "    for (input_ids, input_mask, segment_ids, valid_ids, \n",
    "    label_ids,label_mask) in progress_bar(batched_train_data, \n",
    "    total=pb_max_len, parent=epoch_bar):\n",
    "      loss = train_step(input_ids, input_mask, segment_ids,\n",
    "      valid_ids, label_ids,label_mask)\n",
    "      loss_metric(loss)\n",
    "      epoch_bar.child.comment = f'loss : {loss_metric.result()}'\n",
    "    loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5714a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
